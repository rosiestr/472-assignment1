{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>3. EMBEDDINGS AS FEATURES</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>3.1 Use gensim.downloader.load to load the word2vec-google-news-300 pretrained embedding model.</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "word2vec_googlenews = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>3.2 Use the tokenizer from nltk to extract words from the Reddit posts. Display the number of tokens in the training set.</H4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "2642159\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "import nltk \n",
    "\n",
    "# jsonfiledirectory = \"/Users/rosiers/Documents/GitHub/472-assignment1/goemotions.json.gz\"\n",
    "# jsonfiledirectory = \"C:\\\\Users\\\\Krish\\\\.vscode\\\\472-assignment1\\\\goemotions.json.gz\"\n",
    "jsonfiledirectory = \"G:\\\\My Documents\\\\comp 472\\\\472-assignment1\\\\goemotions.json.gz\"\n",
    "\n",
    "with gzip.open(jsonfiledirectory, \"r\") as f:\n",
    "    data = json.loads(f.read().decode(\"utf-8\"))\n",
    "strings_token = []\n",
    "for item in data:\n",
    "    strings_token.append(nltk.word_tokenize(item[0]))\n",
    "count = 0\n",
    "for str in strings_token:\n",
    "    count = count + len(str)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>3.3 Compute the embedding of a Reddit post as the average of the embeddings of its words. If a word has no embedding in Word2Vec, skip it.</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0625732421875, 0.0309539794921875, 0.0525146484375, 0.18604736328125, -0.09674072265625, 0.0177734375, 0.0715301513671875, -0.0430419921875, -0.009185791015625, -0.009529876708984374, -0.121923828125, -0.18212890625, -0.0642822265625, 0.03070068359375, -0.1669189453125, 0.1212158203125, 0.067041015625, 0.095947265625, 0.0786376953125, -0.1344970703125, -0.0189697265625, 0.01287841796875, 0.184210205078125, 0.0110565185546875, 0.02649974822998047, 0.051744842529296876, -0.115362548828125, 0.043670654296875, 0.01415557861328125, -0.053167724609375, -0.068359375, 0.14326171875, -0.038232421875, -0.06484375, -0.10544891357421875, 0.04044189453125, 0.087939453125, 0.051953125, 0.000640869140625, 0.11961212158203124, 0.04735107421875, -0.04951171875, 0.259423828125, -0.0633392333984375, -0.01748046875, -0.023974609375, 0.02947998046875, -0.10185546875, 0.0018310546875, -0.01885986328125, -0.09895172119140624, 0.0999755859375, -0.00535888671875, 0.06209716796875, 0.0124420166015625, -0.00230712890625, -0.03419036865234375, -0.04813232421875, -0.0088623046875, -0.013018798828125, 0.01180419921875, 0.030804443359375, -0.099658203125, -0.1099853515625, 0.025469970703125, -0.10306396484375, -0.12239990234375, 0.098779296875, -0.1070556640625, 0.00083770751953125, 0.16103515625, 0.1147857666015625, 0.045098876953125, -0.02919921875, -0.2867919921875, -0.140869140625, 0.08260650634765625, 0.107275390625, 0.048114013671875, 0.12018585205078125, -0.072882080078125, -0.017352294921875, 0.02330322265625, 0.009228515625, -0.075921630859375, -0.0625244140625, 0.016351318359375, 0.22568359375, -0.00138092041015625, 0.010997962951660157, 0.0052947998046875, 0.18134765625, -0.0942413330078125, -0.111279296875, -0.02017822265625, -0.05692138671875, 0.1388214111328125, 0.0057861328125, 0.010333251953125, 0.043994140625, -0.128759765625, -0.050413894653320315, -0.014532470703125, 0.052214860916137695, -0.144012451171875, -0.08460845947265624, -0.1317626953125, -0.00274658203125, 0.028350830078125, -0.0848602294921875, -0.135498046875, -0.135711669921875, -0.07398681640625, 0.04893798828125, 0.07822265625, -0.024769973754882813, 0.042498779296875, -0.09033203125, 0.0416259765625, 0.0197265625, -0.1191802978515625, -0.0377777099609375, -0.01749763488769531, 0.1109130859375, -0.0281982421875, -0.06864395141601562, -0.151763916015625, -0.0123046875, 0.0315673828125, -0.0196044921875, -0.14036102294921876, -0.116436767578125, -0.058203125, 0.01986083984375, 0.04188232421875, -0.18193359375, -0.0950927734375, 0.031597900390625, 0.0433837890625, 0.10572052001953125, 0.09912109375, -0.1114501953125, 2.44140625e-05, -0.05406951904296875, 0.09319610595703125, -0.015087890625, -0.0016204833984375, -0.168414306640625, -0.00079345703125, -0.0535003662109375, 0.12933349609375, 0.05709228515625, -0.09605712890625, -0.0371337890625, -0.00335693359375, -0.012408447265625, -0.07098388671875, -0.022168064117431642, -0.0912353515625, 0.02540283203125, 0.01253509521484375, 0.1392333984375, -0.052386474609375, 0.13531036376953126, 0.030023193359375, -0.19501953125, 0.013485336303710937, -0.0671630859375, -0.0607421875, -0.0278564453125, -0.1904296875, 0.005731201171875, 0.008831787109375, -0.0300048828125, -0.05377960205078125, 0.03004150390625, 0.18291015625, -0.10030517578125, -0.038525390625, 0.014902114868164062, -0.10904541015625, -0.0359619140625, 0.03994140625, -0.034356689453125, -0.018743896484375, 0.034906005859375, -0.060107421875, -0.04114990234375, 0.06004638671875, 0.033355712890625, 0.085809326171875, 0.059228515625, 0.005615234375, 0.0198638916015625, 0.06031494140625, 0.021099853515625, -0.06463623046875, -0.0318115234375, -0.020721435546875, -0.075732421875, -0.027880859375, 0.08692626953125, -0.04013824462890625, -0.02333984375, -0.020684814453125, 0.0021728515625, -0.12325439453125, -0.122637939453125, 0.0019775390625, -0.03330078125, -0.0225341796875, 0.08830299377441406, -0.141845703125, 0.05230712890625, -0.18216552734375, -0.0759033203125, 0.164990234375, 0.0138275146484375, -0.11171875, -0.05511474609375, -0.10725936889648438, -0.0187042236328125, -0.0390380859375, -0.054547119140625, 0.12385711669921876, -0.07474365234375, 0.1472900390625, 0.0400146484375, -0.08165283203125, -0.016204833984375, -0.00128173828125, -0.1072021484375, -0.044879150390625, -0.01455078125, 0.08670654296875, 0.028955078125, -0.034765625, 0.045947265625, 0.1225860595703125, 0.00137939453125, -0.0128173828125, 0.00806884765625, 0.05528717041015625, -0.11429443359375, -0.08203125, -0.0048828125, -0.01571044921875, 0.1451202392578125, 0.00057373046875, -0.073876953125, 0.0723388671875, -0.0331298828125, 0.09595947265625, 0.09939756393432617, 0.082196044921875, -0.12406005859375, -0.038983154296875, 0.015087890625, 0.007763671875, -0.071612548828125, -0.0805908203125, -0.016604232788085937, -0.140087890625, 0.0218994140625, 0.044525146484375, 0.08011245727539062, -0.0426971435546875, 0.0084991455078125, -0.0876190185546875, 0.016998291015625, 0.033294677734375, 0.1675048828125, 0.221673583984375, 0.16858062744140626, -0.0201171875, -0.06920166015625, -0.0619720458984375, -0.13974609375, -0.05234375, 0.03576812744140625, 0.039697265625, -0.0860595703125, 0.088262939453125, 0.115185546875, -0.00801544189453125, 0.0279754638671875, -0.0004669189453125, -0.1109375, 0.02270050048828125, 0.0727294921875, -0.01107177734375, 0.15234375, -0.112164306640625, 0.05659828186035156, -0.05213623046875, 0.00498046875, 0.01402587890625, -0.098388671875, -0.0352783203125, -0.00509033203125]\n"
     ]
    }
   ],
   "source": [
    "# create an empty list to add each average embedding to\n",
    "embeddings = []\n",
    "#strings_token is the tokenized sentences\n",
    "for s in strings_token:\n",
    "    #for each sentence create a vector to hold the embeddings for each token in the string\n",
    "    # it will be wiped each time\n",
    "    strings_embedding = []\n",
    "\n",
    "    #for each token in the sentence check if it has embedding and add it to the list or skip\n",
    "    for tkn in s:\n",
    "        if tkn in word2vec_googlenews:\n",
    "            strings_embedding.append(word2vec_googlenews[tkn])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    #use zip to put all elements from each embedding into list of the same index\n",
    "    embd = zip(*strings_embedding)\n",
    "    #calculate the average by finding the sum of the elements at each index, and dividing it by the number of elements\n",
    "    average_embedding = []\n",
    "    for e in embd:\n",
    "        avg = sum(e)/len(e)\n",
    "        average_embedding.append(avg)\n",
    "    #once all the averages are computed, append the average embedding as an element\n",
    "    embeddings.append(average_embedding)\n",
    "print(embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>3.4 Compute and display the overall hit rates of the training and test sets (i.e. the % of words in the Reddit posts for which an embedding is found in Word2Vec).</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall hitrates:\n",
      "0.7745063827339175\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in strings_token: \n",
    "    for tkn in i:\n",
    "        if tkn in word2vec_googlenews:\n",
    "            counter= counter+1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(\"Overall hitrates:\")\n",
    "hitrate= counter/count\n",
    "print(hitrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>3.5 Train a Base-MLP: a Multi-Layered Perceptron (neural network.MLPClassifier) with the default parameters.</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6ecd277e77b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0memotions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mposts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "\n",
    "posts = []\n",
    "sentiment = []\n",
    "emotions = []\n",
    "for item in data:\n",
    "    posts.append(item[0])\n",
    "    sentiment.append(item[2])\n",
    "    emotions.append(item[1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, sentiment, test_size=0.2)\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "#x_numpy = numpy.array(X_train).reshape(-1,1)\n",
    "#y_numpy = numpy.array(y_train).reshape(-1,1)\n",
    "basemlp = MLPClassifier(max_iter=4)\n",
    "basemlp.fit(X_train, y_train)\n",
    "print(basemlp.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>3.6 Train a Top-MLP: a better performing Multi-Layered Perceptron found with whatever hyperparameters you want.</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/rosiers/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-MLP: \n",
      "0.3428879059480852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "topmlp = MLPClassifier(max_iter=5)\n",
    "\n",
    "param_grid = {\n",
    "    'activation' : ['tanh', 'relu', 'identity'],\n",
    "    'hidden_layer_sizes': (20,),\n",
    "    'solver': ['sgd', 'adam']\n",
    "}\n",
    "mlp_grid = GridSearchCV(topmlp, param_grid)\n",
    "mlp_grid.fit(X_train, y_train)\n",
    "print(\"TOP-MLP: \")\n",
    "print(mlp_grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>3.7  Display the performance of your classifiers using metrics.classification report and add these to your performance file.</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>3.8 - OWN EXPLORATION - TWITTER CORPUS<H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import nltk\n",
    "import gzip\n",
    "import json\n",
    "import collections\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize\n",
    "\n",
    "jsonfiledirectory = \"G:\\\\My Documents\\\\comp 472\\\\472-assignment1\\\\goemotions.json.gz\" \n",
    "# word2vec_wiki = KeyedVectors.load_word2vec_format('C:\\\\Users\\\\p_ishna\\\\Downloads\\\\223\\\\model.bin', binary=True)\n",
    "word2vec_twt = gensim.downloader.load(\"glove-twitter-25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642159\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(jsonfiledirectory, \"r\") as f:\n",
    "    data = json.loads(f.read().decode(\"utf-8\"))\n",
    "\n",
    "twts_token = []\n",
    "\n",
    "for item in data:\n",
    "    twts_token.append(nltk.word_tokenize(item[0]))\n",
    "\n",
    "twt_count = 0\n",
    "\n",
    "for str in twts_token:\n",
    "    twt_count = twt_count + len(str)\n",
    "print(twt_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.361672996232907, 0.20551767138143381, -0.053887456306256354, 0.08648266612241666, -0.33824191614985466, 0.01059274934232235, 1.2292008300622304, 0.151105006535848, -0.22960916782418886, 0.23968216187010208, -0.6806558383007845, -0.16538332464794317, -4.8576416571935015, -0.4429346527904272, -0.015779169586797554, 0.009576415022214254, -0.13861807870368162, -0.7982272502655784, -0.2074908266464869, -0.5649208252628645, 0.3374864302653198, 0.4084713359673818, 0.28313133337845403, 0.3909536662201087, 0.08036915957927704]\n"
     ]
    }
   ],
   "source": [
    "# create an empty list to add each average embedding to\n",
    "twt_embeddings = []\n",
    "#strings_token is the tokenized sentences\n",
    "for s in twts_token:\n",
    "    #for each sentence create a vector to hold the embeddings for each token in the string\n",
    "    # it will be wiped each time\n",
    "    twt_embedding = []\n",
    "\n",
    "    #for each token in the sentence check if it has embedding and add it to the list or skip\n",
    "    for tkn in s:\n",
    "        if tkn in word2vec_twt:\n",
    "            twt_embedding.append(word2vec_twt[tkn])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    #use zip to put all elements from each embedding into list of the same index\n",
    "    twtembd = zip(*twt_embedding)\n",
    "    #calculate the average by finding the sum of the elements at each index, and dividing it by the number of elements\n",
    "    twt_average_embedding = []\n",
    "    for e in twtembd:\n",
    "        twt_avg = sum(e)/len(e)\n",
    "        twt_average_embedding.append(twt_avg)\n",
    "    #once all the averages are computed, append the average embedding as an element\n",
    "    twt_embeddings.append(twt_average_embedding)\n",
    "print(twt_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall hitrates:\n",
      "0.8454040805265693\n"
     ]
    }
   ],
   "source": [
    "twt_counter = 0\n",
    "\n",
    "for i in twts_token: \n",
    "    for tkn in i:\n",
    "        if tkn in word2vec_twt:\n",
    "            twt_counter= twt_counter+1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(\"Overall hitrates:\")\n",
    "twt_hitrate= twt_counter/twt_count\n",
    "print(twt_hitrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08256675271938245, -0.0443886686116457, -0.18984775400410095, 0.13071149851505956, -0.19752291667585573, 0.32362952661545324, 1.059941003099084, 0.4997046723340948, -0.35432024595017236, 0.09823083132505417, 0.0009166672825813293, -0.12329499920209248, -4.59116663535436, 0.14156999858096242, -0.05432692061488827, 0.023626004966596763, -0.29123583684364956, -0.14257592086990675, -0.503183338791132, -0.1915783422688643, 0.09286858762303989, -0.08622999923924606, -0.10992558378105362, 0.0433186663625141, -0.0959178883349523]\n",
      "positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-304bff595b25>:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_numpy = numpy.array(X_train).reshape(-1,1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-304bff595b25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0my_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mtwt_mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtwt_mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_numpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwt_mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "\n",
    "posts = []\n",
    "sentiment = []\n",
    "emotions = []\n",
    "for item in data:\n",
    "    posts.append(item[0])\n",
    "    sentiment.append(item[2])\n",
    "    emotions.append(item[1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(twt_embeddings, sentiment, test_size=0.2)\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "#x_numpy = numpy.array(X_train).reshape(-1,1)\n",
    "#y_numpy = numpy.array(y_train).reshape(-1,1)\n",
    "twt_mnb = MultinomialNB()\n",
    "twt_mnb.fit(X_train, y_train)\n",
    "print(twt_mnb.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>3.8 - OWN EXPLORATION - BRITISH NATIONAL CORPUS</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = KeyedVectors.load_word2vec_format('C:\\Users\\b_malpar\\Downloads\\BritishNationalCorpus\\model.bin', binary=True)\n",
    "\n",
    "strings_token = []\n",
    "for item in data:\n",
    "    strings_token.append(nltk.wordpunct_tokenize(item[0]))\n",
    "\n",
    "bncembeddings = []\n",
    "\n",
    "for str in strings_token:\n",
    "    b_embedding = []\n",
    "\n",
    "    for token in str:\n",
    "        if token in newmodel:\n",
    "            b_embedding.append(newmodel[token])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    newembd = zip(*b_embedding)\n",
    "    avg_emb = []\n",
    "\n",
    "    for e in newembd:\n",
    "        avg = sum(e)/len(e)\n",
    "        avg_emb.append(avg)\n",
    "\n",
    "    bncembeddings.append(avg_emb) \n",
    "\n",
    "print(bncembeddings)\n",
    "\n",
    "bnc_counter = 0\n",
    "\n",
    "for s in strings_token: \n",
    "    for tkn in s:\n",
    "        if tkn in newmodel:\n",
    "            bnc_counter= bnc_counter+1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(\"Overall hitrates:\")\n",
    "bnc_hitrate= bnc_counter/count\n",
    "print(bnc_hitrate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

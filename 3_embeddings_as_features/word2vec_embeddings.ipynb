{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[] 3.1 Use gensim.downloader.load to load the word2vec-google-news-300 pretrained embedding model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The corpus_iterable must be an iterable of lists of strings, got <gensim.models.keyedvectors.KeyedVectors object at 0x7f838cd021f0> instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [22], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\n\u001B[1;32m      4\u001B[0m googlenews \u001B[38;5;241m=\u001B[39m gensim\u001B[38;5;241m.\u001B[39mdownloader\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword2vec-google-news-300\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m wvgn \u001B[38;5;241m=\u001B[39m \u001B[43mgensim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWord2Vec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgooglenews\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(wvgn)\n",
      "File \u001B[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/gensim/models/word2vec.py:425\u001B[0m, in \u001B[0;36mWord2Vec.__init__\u001B[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload \u001B[38;5;241m=\u001B[39m call_on_class_only\n\u001B[1;32m    424\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m corpus_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 425\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_corpus_sanity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    426\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_vocab(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, trim_rule\u001B[38;5;241m=\u001B[39mtrim_rule)\n\u001B[1;32m    427\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain(\n\u001B[1;32m    428\u001B[0m         corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, total_examples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count,\n\u001B[1;32m    429\u001B[0m         total_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_total_words, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs, start_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha,\n\u001B[1;32m    430\u001B[0m         end_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_alpha, compute_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n",
      "File \u001B[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/gensim/models/word2vec.py:1500\u001B[0m, in \u001B[0;36mWord2Vec._check_corpus_sanity\u001B[0;34m(self, corpus_iterable, corpus_file, passes)\u001B[0m\n\u001B[1;32m   1498\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParameter corpus_file must be a valid path to a file, got \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m instead\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m corpus_file)\n\u001B[1;32m   1499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(corpus_iterable, Iterable):\n\u001B[0;32m-> 1500\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   1501\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe corpus_iterable must be an iterable of lists of strings, got \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m instead\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m corpus_iterable)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(corpus_iterable, GeneratorType) \u001B[38;5;129;01mand\u001B[39;00m passes \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1503\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   1504\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a generator as corpus_iterable can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt support \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpasses\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m passes. Try a re-iterable sequence.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: The corpus_iterable must be an iterable of lists of strings, got <gensim.models.keyedvectors.KeyedVectors object at 0x7f838cd021f0> instead"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "word2vec_googlenews = gensim.downloader.load('word2vec-google-news-300')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.2 Use the tokenizer from nltk to extract words from the Reddit posts. Display the number of tokens in the training set.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642128\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "import nltk\n",
    "\n",
    "jsonfiledirectory = \"/Users/rosiers/Documents/GitHub/472-assignment1/goemotions.json.gz\"\n",
    "\n",
    "with gzip.open(jsonfiledirectory, \"r\") as f:\n",
    "    data = json.loads(f.read().decode(\"utf-8\"))\n",
    "    strings_token = []\n",
    "for item in data:\n",
    "    strings_token.append(nltk.word_tokenize(item[0]))\n",
    "count = 0\n",
    "for str in strings_token:\n",
    "    count = count + len(str)\n",
    "print(count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.3 Compute the embedding of a Reddit post as the average of the embeddings of its words. If a word has no embedding in Word2Vec, skip it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for str in strings_token:\n",
    "    strings_embedding = []\n",
    "    for tkn in str:\n",
    "        if tkn in word2vec_googlenews:\n",
    "            strings_embedding.append(word2vec_googlenews[tkn])\n",
    "        else:\n",
    "            continue\n",
    "    embd = list(zip(*strings_embedding))\n",
    "    average_embedding = []\n",
    "    for strList in embd:\n",
    "        avg = sum(strList)/len(strList)\n",
    "        average_embedding.append(avg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.4 Compute and display the overall hit rates of the training and test sets (i.e. the % of words in the Reddit posts for which an embedding is found in Word2Vec)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.5 Train a Base-MLP: a Multi-Layered Perceptron (neural network.MLPClassifier) with the default parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.6 Train a Top-MLP: a better performing Multi-Layered Perceptron found with whatever hyperparameters you want."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.7  Display the performance of your classifiers using metrics.classification report and add these to your performance file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
